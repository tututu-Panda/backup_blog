---
title: 批量梯度下降法
tags: [最优化,梯度下降]
toc: true
grammar_cjkRuby: true
grammar_highlight: true
mathjax: true
---

## 是什么？

> 什么是批量梯度下降法？

批量梯度下降根据所有的样本数据，来计算出每个的误差，再根据所有的误差进行更新，去拟合数据函数关系。

<!--more-->


## 为什么？

> 为什么这样做？

1. 原本的梯度下降算法更新的时候，如果样本数据过多，会一个个的进行样本的训练，这样就会很浪费时间、效率以及容易造成错误。

2. 原本的梯度下降法**不能保证找到的极小值点就是函数的极小值点**，然而如果通过**批量的数据对比**，在每一次**去往的梯度方向上进行正确的选择**，那么就可以确保每一次更新都能去往下降最快的方向。

> 实现原理？

1. 首先更新某个点时，用一个样本进行更新时的公式：
$$
\theta_{j} :=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)
$$

其中$$\theta_{j}$$为最开始的随机初始点，$$\alpha$$为下降最大方向的步伐大小。$$\frac{\partial}{\partial \theta_{j}} J(\theta)$$为函数的偏导数。

2. **更新一个点（x，y）时,J的偏导为：**
$$
\begin{aligned} \frac{\partial}{\partial \theta_{j}} J(\theta) &=\frac{\partial}{\partial \theta_{j}} \frac{1}{2}\left(h_{\theta}(x)-y\right)^{2} \\ &=2 \cdot \frac{1}{2}\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(h_{\theta}(x)-y\right) \\ &=\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(\sum_{i=0}^{n} \theta_{i} x_{i}-y\right) \\ &=\left(h_{\theta}(x)-y\right) x_{j} \end{aligned}
$$

对应的**该点更新值为:**
$$
\theta_{j} :=\theta_{j}-\alpha(h_{\theta}(x)-y) x_{j}
$$

3. **再利用其它所有的数据，进行更新,其J的偏导为:**
$$
\frac{\partial J(\theta)}{\partial \theta_{j}}=-\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}
$$

根据所求出的函数值，进行$$\theta_{j}$$的更新，使得$$\theta_{j}$$的值减小到一定值：
$$
\theta_{j}^{\prime}=\theta_{j}+\frac{1}{m} \sum_{i=1}^{m}\left(y^{i}-h_{\theta}\left(x^{i}\right)\right) x_{j}^{i}
$$

4. 一直重复上述过程，**直到满足某个值或者所有样本数据都已经循环完为止。**

## 应用范围？

1. 已知x和y，设计一个函数进行拟合时。
2. 搜索最优解。

## 优缺点？

> 优点

1. 批量梯度下降就是当走到某个点时，用全局已有的数据进行探测，保证每一次更新的梯度方向是正确的。这样就能很好的避免陷入局部极小值中。

> 缺点

1. 每次更新每一个参数时都采用的所有样本数据，因此开销大，时间长。

## 参考链接

[Stochastic vs Batch Gradient Descent](https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1)
[详解梯度下降法的三种形式BGD、SGD以及MBGD](https://zhuanlan.zhihu.com/p/25765735)