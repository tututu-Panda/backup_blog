---
title: 梯度下降法(一)
tags: [最优化,梯度下降]
toc: true
grammar_cjkRuby: true
grammar_highlight: true
mathjax: true
---

## 是什么?

> 什么是梯度?

梯度实质为一个n维向量,表示某一函数在某点处的方向导数沿着该方向取得的最大值.因此沿着梯度的方向变化最快,变化率最大.(百度百科)


$$
grand f= \triangledown f(x_1,x_2,x_3) = ( \frac{\delta f}{\delta x_1} ,\frac{\delta f}{\delta x_2},\frac{\delta f}{\delta x_3} )
$$


<!--more-->
> 什么是梯度下降?

在机器学习算法中,通常需要建立一个目标函数,然后对其进行极小化,找出其极值点.梯度下降便是常用的一种球多元函数极值点的方法.

因为梯度具有最大的变化率,因此要减少目标函数的函数值时,便朝着梯度的负方向前进.通过此种方法找到函数的极值点.

## 为什么?

> 为什么梯度的反方向是函数值局部下降的最快方向?

1.  设当前处于目标函数的某一个点上,该点的坐标为:

$$
\vec{x} = (x_1, x_2, x_3,..., x_n)
$$


则该点的函数值为:

$$
f(\vec{x})
$$


2.  现在要往某个方向进行下降,则设这个随机的方向向量为:

$$
\vec{l} = (l_1,l_2,...,l_n)
$$


则沿着这个方向下降后的函数值为:
$$
f(\vec{x}+\vec{l})
$$


3. 根据多元函数的泰勒展开公式可以得到:

$$
f(\vec{x}+\vec{l}) = f(\vec{x})+\sum_{k=1}^{n}\frac{\delta f}{\delta x_k} \times l_k +o(x)
$$


进一步可以得到他们之间的变化量( o(x)足够小,因此可以忽略 ):

$$
f(\vec{x}+\vec{l})-f(\vec{x}) = \sum_{k=1}^{n}\frac{\delta f}{\delta x_k} \times l_k
$$


4. 其中前者为x点的梯度,后者为随机的方向向量.则可以看出,最终的目的就是求出他们之间的向量积:

$$
\langle\vec{grand f_x}, \vec{l}\rangle = |\vec{grand f_x}| \cdot |\vec{l}| \cdot \cos(\vec{grand f_x},\vec{l})
$$


5. 由上面的式子可以得知,当$cos(\vec{grand f_x},\vec{l})$最小时,$f(\vec{x}+\vec{l})-f(\vec{l})$的值最小.因此$\vec{l}$与$\vec{x}$方向反向时,其取得最小值.从而可以得知$f$沿着负梯度方向前进时下降的最快.

## 有什么用?
> 具体应用范围

1. 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值(百度百科)
2. 待具体实现....


## 优缺点?
1. 靠近极小值时,变化速度会逐渐减慢
2. 可能出现"之"型下降方式
3. 当函数不为凸函数时,可能陷入局部极小值
## 参考链接

[化问题与负梯度方向](https://blog.csdn.net/itplus/article/details/9337515)

[在梯度下降法中，为什么梯度的负方向是函数下降最快的方向？](<https://blog.csdn.net/llwleon/article/details/79237053>)

[梯度下降法](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95#%E7%BC%BA%E7%82%B9)